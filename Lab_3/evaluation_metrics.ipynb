{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c59a71",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in Machine Learning\n",
    "\n",
    "Evaluation metrics help in understanding the performance of machine learning models. This notebook provides an overview of different types of metrics and their implementations using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8919e6b",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Accuracy\n",
    "\n",
    "Accuracy is the ratio of correctly predicted instances to the total instances. It is widely used for classification problems.\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Where:<br>\n",
    "TP: True Positives<br>\n",
    "TN: True Negatives<br>\n",
    "FP: False Positives<br>\n",
    "FN: False Negatives<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77902af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]  # Actual labels\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 1]  # Predicted labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e19abf",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Mean Squared Error (MSE)\n",
    "\n",
    "MSE measures the average squared difference between actual and predicted values, commonly used in regression.\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50fe4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "y_true_reg = [3.0, -0.5, 2.0, 7.0]\n",
    "y_pred_reg = [2.5, 0.0, 2.0, 8.0]\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f5eab",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Mean Absolute Error (MAE)\n",
    "\n",
    "MAE is the average absolute difference between actual and predicted values.\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529db125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8449759",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Precision, Recall, and F1-score\n",
    "\n",
    "These metrics are useful for imbalanced classification problems.\n",
    "\n",
    " **Precision** (Positive Predictive Value) is the fraction of relevant instances among the retrieved instances:\n",
    "  \n",
    "  $$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    " **Recall** (Sensitivity) is the fraction of relevant instances retrieved over the total relevant instances:\n",
    "  \n",
    "  $$ Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    " **F1-score** is the harmonic mean of Precision and Recall:\n",
    "\n",
    "  $$ F1-score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b95774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8333333333333334\n",
      "Recall: 0.8333333333333334\n",
      "F1-score: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78861865",
   "metadata": {},
   "source": [
    "## 5. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "\n",
    "ROC-AUC measures the ability of the classifier to distinguish between classes.<br>\n",
    "\n",
    "ROC Curve plots True Positive Rate vs. False Positive Rate.<br>\n",
    "AUC (Area Under the Curve) represents the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "271b3bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Example binary classification probabilities\n",
    "y_prob = [0.9, 0.1, 0.8, 0.3, 0.2, 0.95, 0.05, 0.6, 0.7, 0.85]\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "print(\"ROC-AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb62974",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<br> **Accuracy** is useful for balanced classification problems but may be misleading in imbalanced datasets.\n",
    "<br> **MSE and MAE** are commonly used for regression models.\n",
    "<br>**Precision, Recall, and F1-score** are useful for imbalanced classification.\n",
    "<br>**ROC-AUC** measures how well the classifier distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f35ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
